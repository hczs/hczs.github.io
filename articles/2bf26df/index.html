<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="HandheldFriendly" content="True"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5"><meta name="description" content="关于 LangChain 的 ChatModels、PromptTemplate、RAG、Agent 相关组件介绍"><meta property="og:type" content="article"><meta property="og:title" content="LangChain 极速入门"><meta property="og:url" content="https://www.powercheng.fun/articles/2bf26df/index.html"><meta property="og:site_name" content="Caiden&#39;s Blog"><meta property="og:description" content="关于 LangChain 的 ChatModels、PromptTemplate、RAG、Agent 相关组件介绍"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.powercheng.fun/articles/2bf26df/image-20250728204436459-3706679.png"><meta property="og:image" content="https://www.powercheng.fun/articles/2bf26df/image-20250728212543380-3709144.png"><meta property="og:image" content="https://www.powercheng.fun/articles/2bf26df/image-20250728214724234-3710445.png"><meta property="og:image" content="https://www.powercheng.fun/articles/2bf26df/image-20250728214909892-3710551.png"><meta property="og:image" content="https://www.powercheng.fun/articles/2bf26df/image-20250728222130875-3712491.png"><meta property="og:image" content="https://www.powercheng.fun/articles/2bf26df/image-20250728222542401-3712744.png"><meta property="article:published_time" content="2025-07-24T12:26:23.000Z"><meta property="article:modified_time" content="2025-07-28T14:45:05.889Z"><meta property="article:author" content="Caiden Hou"><meta property="article:tag" content="LangChain"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://www.powercheng.fun/articles/2bf26df/image-20250728204436459-3706679.png"><link rel="shortcut icon" href="/images/favicon.ico"><link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="stylesheet" href="https://gw.alipayobjects.com/os/k/font/lxgwwenkaiscreenr.css"><title>LangChain 极速入门</title><link rel="stylesheet" href="/css/style.css"><link rel="alternate" href="/true" title="Caiden&#39;s Blog" type="application/atom+xml"><meta name="generator" content="Hexo 5.4.0"></head><body class="max-width mx-auto px3 ltr"><div id="header-post"><a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a> <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a> <a id="top-icon-tablet" href="#" aria-label="顶部" onclick='$("html, body").animate({scrollTop:0},"fast")' style="display:none"><i class="fa-solid fa-chevron-up fa-lg"></i></a> <span id="menu"><span id="nav"><ul><li><a href="/">首页</a></li><li><a href="/archives/">文章</a></li><li><a href="/categories/">分类</a></li><li><a href="/search/">搜索</a></li><li><a href="/about/">关于</a></li></ul></span><br><span id="actions"><ul><li><a class="icon" aria-label="下一篇" href="/articles/3322b824/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class="icon" aria-label="返回顶部" href="#" onclick='$("html, body").animate({scrollTop:0},"fast")'><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover='$("#i-share").toggle()' onmouseout='$("#i-share").toggle()' onclick='return $("#share").toggle(),!1'></i></a></li></ul><span id="i-prev" class="info" style="display:none">上一篇</span> <span id="i-next" class="info" style="display:none">下一篇</span> <span id="i-top" class="info" style="display:none">返回顶部</span> <span id="i-share" class="info" style="display:none">分享文章</span></span><br><div id="share" style="display:none"><ul><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.facebook.com/sharer.php?u=https://www.powercheng.fun/articles/2bf26df/"><i class="fab fa-facebook" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://twitter.com/share?url=https://www.powercheng.fun/articles/2bf26df/&text=LangChain 极速入门"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.linkedin.com/shareArticle?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://pinterest.com/pin/create/bookmarklet/?url=https://www.powercheng.fun/articles/2bf26df/&is_video=false&description=LangChain 极速入门"><i class="fab fa-pinterest" aria-hidden="true"></i></a></li><li><a class="icon" href="mailto:?subject=LangChain 极速入门&body=Check out this article: https://www.powercheng.fun/articles/2bf26df/" rel="external nofollow noreferrer"><i class="fa-solid fa-envelope" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://getpocket.com/save?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-get-pocket" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://reddit.com/submit?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-reddit" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.stumbleupon.com/submit?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-stumbleupon" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://digg.com/submit?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-digg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" href="http://www.tumblr.com/share/link?url=https://www.powercheng.fun/articles/2bf26df/&name=LangChain 极速入门&description=&lt;link rel=&#34;stylesheet&#34; type=&#34;text/css&#34; href=&#34;https://cdn.jsdelivr.net/lightgallery.js/1.0.1/css/lightgallery.min.css&#34; /&gt;&lt;div class=&#34;.article-gallery&#34;&gt;&lt;p&gt;关于 LangChain 的 ChatModels、PromptTemplate、RAG、Agent 相关组件介绍&lt;/p&gt;"><i class="fab fa-tumblr" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://news.ycombinator.com/submitlink?u=https://www.powercheng.fun/articles/2bf26df/&t=LangChain 极速入门"><i class="fab fa-hacker-news" aria-hidden="true"></i></a></li></ul></div><div id="toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">1. 整体介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-ChatModels"><span class="toc-number">2.</span> <span class="toc-text">2. ChatModels</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 基本使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%B8%A6%E6%9C%89%E5%8E%86%E5%8F%B2%E6%B6%88%E6%81%AF%E7%9A%84%E5%AF%B9%E8%AF%9D"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 带有历史消息的对话</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%8A%A8%E6%80%81%E6%9E%84%E5%BB%BA%E5%8E%86%E5%8F%B2%E6%B6%88%E6%81%AF"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 动态构建历史消息</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-PromptTemplate"><span class="toc-number">3.</span> <span class="toc-text">3. PromptTemplate</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-from-template"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 from_template</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-from-messages"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 from_messages</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Chain"><span class="toc-number">4.</span> <span class="toc-text">4. Chain</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 基础使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E8%87%AA%E5%AE%9A%E4%B9%89-Chain-%E8%8A%82%E7%82%B9"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 自定义 Chain 节点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%B9%B6%E8%A1%8C-Chain"><span class="toc-number">4.3.</span> <span class="toc-text">4.2 并行 Chain</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%88%86%E6%94%AF-Chain"><span class="toc-number">4.4.</span> <span class="toc-text">4.3 分支 Chain</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-RAG"><span class="toc-number">5.</span> <span class="toc-text">5. RAG</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Agent"><span class="toc-number">6.</span> <span class="toc-text">6. Agent</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-ReAct"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 ReAct</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-LangChain-%E7%9A%84-ReAct-Agent"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 LangChain 的 ReAct Agent</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">7.</span> <span class="toc-text">7. 参考链接</span></a></li></ol></div></span></div><div class="content index py4"><article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"><header><h1 class="posttitle p-name" itemprop="name headline">LangChain 极速入门</h1><div class="meta"><div class="postdate"><time datetime="2025-07-24T12:26:23.000Z" class="dt-published" itemprop="datePublished">2025-07-24</time> (Updated: <time datetime="2025-07-28T14:45:05.889Z" class="dt-updated" itemprop="dateModified">2025-07-28</time>)</div><div class="article-category"><i class="fa-solid fa-archive"></i> <a class="category-link" href="/categories/AI/">AI</a></div><div class="article-tag"><i class="fa-solid fa-tag"></i> <a class="p-category" href="/tags/LangChain/" rel="tag">LangChain</a></div></div></header><div class="content e-content" itemprop="articleBody"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/lightgallery.js/1.0.1/css/lightgallery.min.css"><div class=".article-gallery"><p>关于 LangChain 的 ChatModels、PromptTemplate、RAG、Agent 相关组件介绍</p><span id="more"></span><h1 id="1-整体介绍"><a href="#1-整体介绍" class="headerlink" title="1. 整体介绍"></a>1. 整体介绍</h1><p>LangChain 其实用途很简单，就是一个框架，让我们方便的构建基于大语言模型（LLM）的框架，让 LLM 可以方便的和外部数据、工具、逻辑结合，从而构建出功能更强，上下文更丰富的应用。</p><ul><li>我们需要对接各个厂商的 LLM，所以就有了<code>ChatModels</code>组件，想用哪个厂商的就 install 哪个厂商的包，然后 import 对应的 ChatXXX 类就行；</li><li>我们需要写提示词（Prompt），所以就有了<code>PromptTemplate</code> 组件，可以写系统提示词，灵活替换变量；</li><li>我们需要在 LLM 调用前后加一些逻辑，或者调用 A 模型处理 XXX，然后调用 B 模型处理 XXX，所以有了 <code>Chain</code> 组件，把一次任务作为一个 workflow 串联起来；</li><li>我们需要处理大模型幻觉问题，想用外部数据为大模型的知识做补充，所以有了 <code>RAG</code>；</li><li>RAG 需要加载文档、拆分文档，所以有了各种<code>XXXLoader</code> 组件，也提供了各种 <code>XXXSplitter</code>；</li><li>加载拆分好的文档，需要方便的根据问题进行相似度搜索，所以 LangChain 又提供了各种向量库的集成（存储 &amp; 相似度搜索）以及<code>Embeddings</code>模型的集成；</li><li>我们需要 LLM 自行思考行动，所以 LangChain 也提供了 Agent 支持，并且可以很方便的初始化一个 ReAct 模式的 Agent。</li></ul><a href="/articles/2bf26df/image-20250728204436459-3706679.png" class="gallery-item"><img src="/articles/2bf26df/image-20250728204436459-3706679.png" title="image-20250728204436459"></a><h1 id="2-ChatModels"><a href="#2-ChatModels" class="headerlink" title="2. ChatModels"></a>2. ChatModels</h1><a href="/articles/2bf26df/image-20250728212543380-3709144.png" class="gallery-item"><img src="/articles/2bf26df/image-20250728212543380-3709144.png" title="image-20250728212543380"></a><h2 id="2-1-基本使用"><a href="#2-1-基本使用" class="headerlink" title="2.1 基本使用"></a>2.1 基本使用</h2><p>其实做通用的就是 <code>ChatOpenAI</code> 类，也就是安装 <code>langchain-openai</code> 依赖包，这个 <code>ChatOpenAI</code> 就是兼容 OpenAI API 风格的厂商接口，现在大部分厂商都会提供 OpenAI API 风格的接口比如硅基流动、魔塔等等。</p><p>下面是国内可用的方案，用硅基流动的 api 来跑通 LLM 请求逻辑（需要在 .env 文件中写入 <code>OPENAI_API_KEY</code> 变量）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 兼容 openai api 的服务商都能用 ChatOpenAI</span></span><br><span class="line"><span class="comment"># 要设置 OPENAI_API_KEY 环境变量</span></span><br><span class="line">model = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果想选择其他供应商的模型</span></span><br><span class="line"><span class="comment"># https://python.langchain.com/docs/integrations/providers/</span></span><br><span class="line"><span class="comment"># 基本用下来就是 ChatXXX</span></span><br><span class="line"><span class="comment"># 例如 pip install -U langchain-anthropic ChatAnthropic</span></span><br><span class="line"><span class="comment"># pip install langchain-ollama 的 ChatOllama 类 都是类似的用法</span></span><br><span class="line"></span><br><span class="line">response = model.invoke(<span class="string">&quot;你好啊！你是谁呢！&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">你好！我是Qwen，我是阿里云研发的超大规模语言模型，现在可以回答你的问题，创作文字，比如写故事、写公文、写邮件、写剧本等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，尽管告诉我，我会尽力提供支持！</span><br></pre></td></tr></table></figure><h2 id="2-2-带有历史消息的对话"><a href="#2-2-带有历史消息的对话" class="headerlink" title="2.2 带有历史消息的对话"></a>2.2 带有历史消息的对话</h2><p><code>invoke</code> 方法很灵活，不止可以传入一个字符串消息，也可以传入一个消息列表，这样 LLM 就会记得之前的聊天记录～</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, SystemMessage</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(</span><br><span class="line">        content=<span class="string">&quot;你的名字叫豆包，你的任务是回答用户的问题。当用户问你是谁时，你应该说你是豆包！&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;你好啊！你是谁呢！&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = llm.invoke(messages)</span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">你好！我是豆包！有什么问题我可以帮助你解答吗？</span><br></pre></td></tr></table></figure><h2 id="2-3-动态构建历史消息"><a href="#2-3-动态构建历史消息" class="headerlink" title="2.3 动态构建历史消息"></a>2.3 动态构建历史消息</h2><p>就是把上面的 messages 存起来了，每次对话都发给 LLM 完整的消息列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line">chat_history = []</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    query = <span class="built_in">input</span>(<span class="string">&quot;请输入你的问题: &quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> query.lower() <span class="keyword">in</span> [<span class="string">&quot;exit&quot;</span>, <span class="string">&quot;quit&quot;</span>]:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;退出对话。&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    chat_history.append(HumanMessage(content=query))</span><br><span class="line">    response = llm.invoke(chat_history)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;AI: <span class="subst">&#123;response.content&#125;</span>&quot;</span>)</span><br><span class="line">    chat_history.append(response)</span><br></pre></td></tr></table></figure><p>测试效果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">请输入你的问题: 我的名字叫 caiden </span><br><span class="line">AI: 很高兴认识你，Caiden！有什么我可以帮助你的吗？</span><br><span class="line">请输入你的问题: 我叫啥?</span><br><span class="line">AI: 你叫 Caiden。</span><br><span class="line">请输入你的问题: </span><br></pre></td></tr></table></figure><h1 id="3-PromptTemplate"><a href="#3-PromptTemplate" class="headerlink" title="3. PromptTemplate"></a>3. PromptTemplate</h1><a href="/articles/2bf26df/image-20250728214724234-3710445.png" class="gallery-item"><img src="/articles/2bf26df/image-20250728214724234-3710445.png" title="image-20250728214724234"></a><h2 id="3-1-from-template"><a href="#3-1-from-template" class="headerlink" title="3.1 from_template"></a>3.1 from_template</h2><p>这个分成三部分：</p><ul><li>原始模版纯字符串，里面变量使用<code>&#123;xxx&#125;</code>这样的格式设置</li><li><code>ChatPromptTemplate.from_template(template)</code> 基于原始模版的字符串实例化出一个 <code>ChatPromptTemplate</code>对象</li><li>用实例化后的对象<code>invoke</code>传入定义好的变量（字典类型），然后获取到最终的 message 对象，可以发给 LLM 使用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">你是Kimi,我的身份是&#123;role&#125;,你的任务:&#123;task&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">prompt_template = ChatPromptTemplate.from_template(template)</span><br><span class="line"></span><br><span class="line">message = prompt_template.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;用户&quot;</span>,</span><br><span class="line">        <span class="string">&quot;task&quot;</span>: <span class="string">&quot;回答用户的问题。当用户问你是谁时,你应该说你是Kimi&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实际invoke后是一个human message 无法自定义身份</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;实际msg: <span class="subst">&#123;message&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line">model = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = model.invoke(message)</span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">实际msg: messages=[HumanMessage(content=<span class="string">&#x27;\n你是Kimi,我的身份是Kimi,你的任务:回答用户的问题。当用户问你是谁时,你应该说你是Kimi\n&#x27;</span>, additional_kwargs=&#123;&#125;, response_metadata=&#123;&#125;)]</span><br><span class="line">你好，但我并不是Kimi。我是Qwen，由阿里云创造的大型语言模型，旨在提供帮助和交流。如果你有其他问题或者需要帮助，我很乐意为你提供支持。如果你还是希望以Kimi的身份交流，可以继续告诉我，我会尽量按照你的要求来。不过，请记得我真正的身份是Qwen。</span><br></pre></td></tr></table></figure><p>可以发现这个实际 invoke 后的对象是一个 <code>HumanMessage</code>对象，无法设置系统提示词，需要设置系统提示词+用户提示词，需要用到<code>from_messages</code>方法。</p><h2 id="3-2-from-messages"><a href="#3-2-from-messages" class="headerlink" title="3.2 from_messages"></a>3.2 from_messages</h2><p>这个是用 <code>from_messages</code> 实例化出 template 对象，然后 invoke 正常传递值就行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    (</span><br><span class="line">        <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;你是&#123;role&#125;，你的任务是回答用户的问题。当用户问你是谁时,你应该说你是&#123;name&#125;&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;你好啊！我的身份是&#123;user_role&#125;你是谁呢！&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages(messages)</span><br><span class="line">message = prompt_template.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;Kimi&quot;</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;智能助手 Kimi&quot;</span>, <span class="string">&quot;user_role&quot;</span>: <span class="string">&quot;用户&quot;</span>&#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 实际msg 是一个 msg 列表,填充了值了</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;实际msg: <span class="subst">&#123;message&#125;</span>&quot;</span>)</span><br><span class="line">model = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line">response = model.invoke(message)</span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">实际msg: messages=[SystemMessage(content=<span class="string">&#x27;你是Kimi，你的任务是回答用户的问题。当用户问你是谁时,你应该说你是智能助手 Kimi&#x27;</span>, additional_kwargs=&#123;&#125;, response_metadata=&#123;&#125;), HumanMessage(content=<span class="string">&#x27;你好啊！我的身份是用户你是谁呢！&#x27;</span>, additional_kwargs=&#123;&#125;, response_metadata=&#123;&#125;)]</span><br><span class="line">你好！我是智能助手Kimi，我在这里帮助你解答问题和提供帮助。有什么我可以为你做的吗？</span><br></pre></td></tr></table></figure><p>可以看到现在的身份已经可以正常设置了。</p><h1 id="4-Chain"><a href="#4-Chain" class="headerlink" title="4. Chain"></a>4. Chain</h1><a href="/articles/2bf26df/image-20250728214909892-3710551.png" class="gallery-item"><img src="/articles/2bf26df/image-20250728214909892-3710551.png" title="image-20250728214909892"></a><h2 id="4-1-基础使用"><a href="#4-1-基础使用" class="headerlink" title="4.1 基础使用"></a>4.1 基础使用</h2><p>用 chain 可以简化我们的操作，之前我们都需要定义好 prompt template 之后，手动 invoke 出来消息，然后传入 LLM 再接收响应，再做其他操作，这样写起来非常的冗余，所以 LangChain 就提出了 Chain 这个组件。</p><ul><li>通过管道操作符<code>｜</code>链接每个步骤节点</li><li>上个节点的输出就是下个节点的输入</li><li>chain 的 invoke 方法，参数所有节点都共享，并不是只有第一个节点能接收到</li></ul><p>这是一个用 Chain 做一个翻译助手的 demo：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个&#123;role&#125;, 你的任务是帮助用户解决问题,并给出建议&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;你好啊！我的身份是&#123;user_role&#125;,你帮我完成这个任务:&#123;task&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上一步的输出 传入给下一步 依次运行</span></span><br><span class="line">chain = prompt_template | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># invoke 的参数,并不是只有第一个节点可以使用,后面如果 prompt template,也可以用到这些参数</span></span><br><span class="line">response = chain.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;翻译助手&quot;</span>,</span><br><span class="line">        <span class="string">&quot;user_role&quot;</span>: <span class="string">&quot;学生&quot;</span>,</span><br><span class="line">        <span class="string">&quot;task&quot;</span>: <span class="string">&quot;将这段话翻译成英文: 你好，世界！&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">你好！作为学生，你的任务我来帮你完成。这段话翻译成英文是： <span class="string">&quot;Hello, world!&quot;</span> </span><br><span class="line"></span><br><span class="line">这句话常用于编程和语言学习中，作为最基本的打招呼方式。希望这对你有帮助！如果还有其他问题，随时可以问我。</span><br></pre></td></tr></table></figure><h2 id="4-2-自定义-Chain-节点"><a href="#4-2-自定义-Chain-节点" class="headerlink" title="4.2 自定义 Chain 节点"></a>4.2 自定义 Chain 节点</h2><p>chain 的内部原理其实就是：</p><ul><li>chain 就是一个 RunnableSequence 实例对象</li><li>RunnableSequence 中都是节点，可以是 RunnableParallel、RunnableSequence、RunnableLambda 等等</li></ul><p>也就是 chain 可以后面接一个 chain，如果我们想要<strong>自定义一个方法作为 chain 的节点，可以使用 <code>RunnableLambda</code>这个类进行包装一下</strong>，下面是具体的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableLambda, RunnableSequence</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个&#123;role&#125;, 你的任务是帮助用户解决问题,并给出建议&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;你好啊！我的身份是&#123;user_role&#125;,你帮我完成这个任务:&#123;task&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">format_prompt</span>(<span class="params">inputs: <span class="built_in">dict</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> prompt_template.invoke(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以自定义 chain 节点</span></span><br><span class="line"><span class="built_in">format</span> = RunnableLambda(func=format_prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等同于</span></span><br><span class="line"><span class="comment"># chain = format | llm | StrOutputParser</span></span><br><span class="line"><span class="comment"># 管道连接符底层原理其实就是 RunnableSequence</span></span><br><span class="line">chain = RunnableSequence(first=<span class="built_in">format</span>, middle=[llm], last=StrOutputParser())</span><br><span class="line">response = chain.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;翻译助手&quot;</span>,</span><br><span class="line">        <span class="string">&quot;user_role&quot;</span>: <span class="string">&quot;学生&quot;</span>,</span><br><span class="line">        <span class="string">&quot;task&quot;</span>: <span class="string">&quot;将这段话翻译成英文: 你好，世界！&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Hello, world!</span><br></pre></td></tr></table></figure><h2 id="4-2-并行-Chain"><a href="#4-2-并行-Chain" class="headerlink" title="4.2 并行 Chain"></a>4.2 并行 Chain</h2><p>比如我们有个场景，先总结文本，分别翻译成英语和法语，需要同时并行两个 LLM 做翻译工作，当然这个场景不一定合理，但是肯定有并行运行任务的需求，所以就有了并行的 Chain，也就是在 Chain 中添加 <code>RunnableParallel</code>实例对象即可，下面是具体 demo：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 chain 中加入一个 RunnableParallel 实例</span></span><br><span class="line"><span class="comment"># demo: 先总结文本,然后将总结翻译成英语和法语两个版本</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableLambda, RunnableParallel</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个&#123;role&#125;, 你的任务是帮助用户解决问题,并给出建议&quot;</span>),</span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;你好啊！我的身份是&#123;user_role&#125;,你帮我完成这个任务:&#123;task&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">summary_chain = prompt_template | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_en_msg</span>(<span class="params">inputs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;将以下内容翻译成英文: <span class="subst">&#123;inputs&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">translate_en_chain = translate_en_msg | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_fr_msg</span>(<span class="params">inputs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;将以下内容翻译成法语: <span class="subst">&#123;inputs&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">translate_fr_chain = translate_fr_msg | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line">parallel_chain = RunnableParallel(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;en&quot;</span>: translate_en_chain,</span><br><span class="line">        <span class="string">&quot;fr&quot;</span>: translate_fr_chain,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_translations</span>(<span class="params">inputs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;英文翻译: <span class="subst">&#123;inputs[<span class="string">&#x27;en&#x27;</span>]&#125;</span>\n法语翻译: <span class="subst">&#123;inputs[<span class="string">&#x27;fr&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># final chain</span></span><br><span class="line">summary_translate_chain = (</span><br><span class="line">    summary_chain | parallel_chain | RunnableLambda(func=combine_translations)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = summary_translate_chain.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;总结助手&quot;</span>,</span><br><span class="line">        <span class="string">&quot;user_role&quot;</span>: <span class="string">&quot;主理人&quot;</span>,</span><br><span class="line">        <span class="string">&quot;task&quot;</span>: <span class="string">&quot;我今天早上喝了杯牛奶,吃了一个三明治,你帮我总结一下今天早上我干了什么&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">英文翻译: Hello! Based on your description, this morning you drank a glass of milk <span class="keyword">and</span> ate a sandwich. To summarize briefly: you had breakfast, choosing milk <span class="keyword">and</span> a sandwich <span class="keyword">as</span> your breakfast items. I hope this summary <span class="keyword">is</span> helpful to you. If you have <span class="built_in">any</span> other content that needs summarizing <span class="keyword">or</span> <span class="keyword">if</span> you need further advice, feel free to let me know!</span><br><span class="line">法语翻译: Bonjour ! D<span class="string">&#x27;après ta description, ce matin, tu as bu un verre de lait et mangé un sandwich. Pour résumer simplement : tu as pris ton petit-déjeuner, en choisissant du lait et un sandwich comme repas du matin. J&#x27;</span>espère que ce ré<span class="built_in">sum</span>é te sera utile. Si tu <span class="keyword">as</span> d<span class="string">&#x27;autres points à résumer ou si tu as besoin de conseils supplémentaires, n&#x27;</span>hésite pas à me le faire savoir !</span><br></pre></td></tr></table></figure><p>RunnableParallel 接收传入一个字典，key 随意自定义，value 必须是 RunnableXXX，示例中就是传入了两个 chain，最后每个 chain 输出的内容就是根据 key 获取的，见 <code>combine_translations</code>方法。</p><h2 id="4-3-分支-Chain"><a href="#4-3-分支-Chain" class="headerlink" title="4.3 分支 Chain"></a>4.3 分支 Chain</h2><p>这个其实就是在 chain 中拼接 RunnableBranch 实例，下面是一个情感分析的 demo，先分析用户的情感，根据用户不同的情绪，给出不同的回复：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 就是在 chain 中拼接 RunnableBranch 实例</span></span><br><span class="line"><span class="comment"># 情感分析,先分析用户的情感,然后根据情感给出不同的回复</span></span><br><span class="line"><span class="comment"># 如果用户情感积极 就积极的回复</span></span><br><span class="line"><span class="comment"># 如果用户情感消极 就消极的回复</span></span><br><span class="line"><span class="comment"># 如果用户情感自然 就自然的回复</span></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableBranch, RunnableLambda</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>, base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 情感分析 chain</span></span><br><span class="line">sentiment_analysis_chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;分析以下内容的情感(积极/消极/自然):&#123;text&#125;&quot;</span>)</span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 积极回复 chain</span></span><br><span class="line">positive_response_chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;请以积极的情感回复用户,用户说的是:&#123;text&#125;&quot;</span>)</span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 消极回复 chain</span></span><br><span class="line">negative_response_chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;请以消极的情感回复用户,用户说的是:&#123;text&#125;&quot;</span>)</span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自然回复 chain</span></span><br><span class="line">natural_response_chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;请以自然的情感回复用户,用户说的是:&#123;text&#125;&quot;</span>)</span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分支处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">default_response</span>(<span class="params">inputs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;识别结果:<span class="subst">&#123;inputs&#125;</span>; \n 抱歉，我无法理解您的情感。请再说一遍。&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">branches = RunnableBranch(</span><br><span class="line">    (<span class="keyword">lambda</span> x: <span class="string">&quot;积极&quot;</span> <span class="keyword">in</span> <span class="built_in">str</span>(x), positive_response_chain),</span><br><span class="line">    (<span class="keyword">lambda</span> x: <span class="string">&quot;消极&quot;</span> <span class="keyword">in</span> <span class="built_in">str</span>(x), negative_response_chain),</span><br><span class="line">    (<span class="keyword">lambda</span> x: <span class="string">&quot;自然&quot;</span> <span class="keyword">in</span> <span class="built_in">str</span>(x), natural_response_chain),</span><br><span class="line">    RunnableLambda(func=default_response),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终 chain</span></span><br><span class="line">final_chain = sentiment_analysis_chain | branches</span><br><span class="line"></span><br><span class="line">response = final_chain.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;text&quot;</span>: <span class="string">&quot;好吧,我有点饿了&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">你说得对，这句话确实传达了一种平和而自然的情感状态。它就像是一阵轻柔的风，静静地述说着一个简单而真实的感受。没有大起大落的情绪，只是淡淡地表达了当下的一个小小需求。这种自然流露的感觉，往往能让人感到真实和亲切。</span><br></pre></td></tr></table></figure><p>RunnableBranch 实例化的时候接收多个元组，或者是 RunnableXXX，元组中第一个必须是返回布尔值的一个方法，第二个是 RunnableXXX，即<code>(condition, Runnable)</code> ，判断 condition 的方法输入就是 branch 节点上一个节点的输出内容，根据输入内容写判断逻辑即可～</p><h1 id="5-RAG"><a href="#5-RAG" class="headerlink" title="5. RAG"></a>5. RAG</h1><a href="/articles/2bf26df/image-20250728222130875-3712491.png" class="gallery-item"><img src="/articles/2bf26df/image-20250728222130875-3712491.png" title="image-20250728222130875"></a><p>RAG 里面大概几个组件就是：</p><ul><li>数据加载（xxxLoader）</li><li>数据分割（xxxSplitter）</li><li>向量化（嵌入模型）</li><li>向量存储（向量库的不同，引入的包也不同）</li><li>相似度搜索（不同的库搜索方式其实都是大同小异）</li><li>AI回答<ul><li>检索到的相似块 + 用户 query 放到 prompt 中</li><li>通过 invoke 后的 msg 出入 llm</li><li>llm 做出响应回答，完成 RAG</li></ul></li></ul><p>强烈推荐看官方文档：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://python.langchain.com/docs/tutorials/rag/">https://python.langchain.com/docs/tutorials/rag/</a></p><h1 id="6-Agent"><a href="#6-Agent" class="headerlink" title="6. Agent"></a>6. Agent</h1><a href="/articles/2bf26df/image-20250728222542401-3712744.png" class="gallery-item"><img src="/articles/2bf26df/image-20250728222542401-3712744.png" title="image-20250728222542401"></a><h2 id="6-1-ReAct"><a href="#6-1-ReAct" class="headerlink" title="6.1 ReAct"></a>6.1 ReAct</h2><p>ReAct 是让 LLM 在执行任务时更有逻辑，更能调用工具、更像人类思考的<strong>提示词策略</strong>，用于构建推理+行动的 Agent。</p><p>这个是 LangChain 官方提供的一个提示词：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Answer the following questions as best you can. You have access to the following tools:</span><br><span class="line"></span><br><span class="line">&#123;tools&#125;</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line"></span><br><span class="line">Question: the input question you must answer</span><br><span class="line">Thought: you should always think about what to <span class="keyword">do</span></span><br><span class="line">Action: the action to take, should be one of [&#123;tool_names&#125;]</span><br><span class="line">Action Input: the input to the action</span><br><span class="line">Observation: the result of the action</span><br><span class="line">... (this Thought/Action/Action Input/Observation can repeat N <span class="built_in">times</span>)</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: the final answer to the original input question</span><br><span class="line"></span><br><span class="line">Begin!</span><br><span class="line"></span><br><span class="line">Question: &#123;input&#125;</span><br><span class="line">Thought:&#123;agent_scratchpad&#125;</span><br></pre></td></tr></table></figure><p>可以看到就是让 LLM 先思考，再采取动作，再观察结果，再思考，重复 N 次直到观察到最终答案。</p><h2 id="6-2-LangChain-的-ReAct-Agent"><a href="#6-2-LangChain-的-ReAct-Agent" class="headerlink" title="6.2 LangChain 的 ReAct Agent"></a>6.2 LangChain 的 ReAct Agent</h2><p>LangChain 提供了 create_react_agent 这样一个方法，帮助我们创建 ReAct Agent，示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_react_agent, tool</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    base_url=<span class="string">&quot;https://api.siliconflow.cn/v1&quot;</span>, model=<span class="string">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">prompt_template = hub.pull(<span class="string">&quot;hwchase17/react&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_system_time</span>(<span class="params"><span class="built_in">format</span>: <span class="built_in">str</span> = <span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the current date and time in the specified format&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    current_time = datetime.datetime.now()</span><br><span class="line">    formatted_time = current_time.strftime(<span class="built_in">format</span>)</span><br><span class="line">    <span class="keyword">return</span> formatted_time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tools = [get_system_time]</span><br><span class="line"></span><br><span class="line">agent = create_react_agent(llm=llm, prompt=prompt_template, tools=tools)</span><br><span class="line"></span><br><span class="line">executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">executor.invoke(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;现在几点钟了?&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&gt; Entering new AgentExecutor chain...</span><br><span class="line">我需要获取当前的时间来回答这个问题。</span><br><span class="line">Action: get_system_time</span><br><span class="line">Action Input: <span class="string">&#x27;%H:%M&#x27;</span><span class="string">&#x27;22:35&#x27;</span>我现知道了当前的时间。</span><br><span class="line">Final Answer: 现在是<span class="number">22</span>点<span class="number">35</span>分。</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br></pre></td></tr></table></figure><p>可以看到 LLM 就是先思考，再采取行动，再观察结果，发现是获取到问题结果了，就输出最终答案。</p><p>create_react_agent 需要传入 llm、prompt、tools 参数，创建一个 agent 对象；</p><p>AgentExecutor 需要传入 agent 和 tools，verbose 是输出执行过程的详细信息，这样就能创建一个 executor 对象，可以 invoke 执行 agent。</p><p>这就是 LangChain 的 Agent Demo。</p><h1 id="7-参考链接"><a href="#7-参考链接" class="headerlink" title="7. 参考链接"></a>7. 参考链接</h1><ol><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://python.langchain.com/docs/tutorials/">https://python.langchain.com/docs/tutorials/</a></li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://python.langchain.com/docs/tutorials/llm_chain/">https://python.langchain.com/docs/tutorials/llm_chain/</a></li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://python.langchain.com/docs/tutorials/retrievers/">https://python.langchain.com/docs/tutorials/retrievers/</a></li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://python.langchain.com/docs/tutorials/agents/">https://python.langchain.com/docs/tutorials/agents/</a></li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://python.langchain.com/docs/tutorials/rag/">https://python.langchain.com/docs/tutorials/rag/</a></li><li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://python.langchain.com/docs/tutorials/qa_chat_history/">https://python.langchain.com/docs/tutorials/qa_chat_history/</a></li></ol></div><script src="https://cdn.jsdelivr.net/lightgallery.js/1.0.1/js/lightgallery.min.js"></script><script>var options;"undefined"!=typeof lightGallery&&(options={selector:".gallery-item"},lightGallery(document.getElementsByClassName(".article-gallery")[0],options))</script></div></article><div class="blog-post-comments"><div id="disqus_thread"><noscript>加载评论需要在浏览器启用 JavaScript 脚本支持。</noscript></div></div><div id="footer-post-container"><div id="footer-post"><div id="nav-footer" style="display:none"><ul><li><a href="/">首页</a></li><li><a href="/archives/">文章</a></li><li><a href="/categories/">分类</a></li><li><a href="/search/">搜索</a></li><li><a href="/about/">关于</a></li></ul></div><div id="toc-footer" style="display:none"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">1. 整体介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-ChatModels"><span class="toc-number">2.</span> <span class="toc-text">2. ChatModels</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 基本使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%B8%A6%E6%9C%89%E5%8E%86%E5%8F%B2%E6%B6%88%E6%81%AF%E7%9A%84%E5%AF%B9%E8%AF%9D"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 带有历史消息的对话</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%8A%A8%E6%80%81%E6%9E%84%E5%BB%BA%E5%8E%86%E5%8F%B2%E6%B6%88%E6%81%AF"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 动态构建历史消息</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-PromptTemplate"><span class="toc-number">3.</span> <span class="toc-text">3. PromptTemplate</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-from-template"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 from_template</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-from-messages"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 from_messages</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Chain"><span class="toc-number">4.</span> <span class="toc-text">4. Chain</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 基础使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E8%87%AA%E5%AE%9A%E4%B9%89-Chain-%E8%8A%82%E7%82%B9"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 自定义 Chain 节点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%B9%B6%E8%A1%8C-Chain"><span class="toc-number">4.3.</span> <span class="toc-text">4.2 并行 Chain</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%88%86%E6%94%AF-Chain"><span class="toc-number">4.4.</span> <span class="toc-text">4.3 分支 Chain</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-RAG"><span class="toc-number">5.</span> <span class="toc-text">5. RAG</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Agent"><span class="toc-number">6.</span> <span class="toc-text">6. Agent</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-ReAct"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 ReAct</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-LangChain-%E7%9A%84-ReAct-Agent"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 LangChain 的 ReAct Agent</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">7.</span> <span class="toc-text">7. 参考链接</span></a></li></ol></div><div id="share-footer" style="display:none"><ul><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.facebook.com/sharer.php?u=https://www.powercheng.fun/articles/2bf26df/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://twitter.com/share?url=https://www.powercheng.fun/articles/2bf26df/&text=LangChain 极速入门"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.linkedin.com/shareArticle?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://pinterest.com/pin/create/bookmarklet/?url=https://www.powercheng.fun/articles/2bf26df/&is_video=false&description=LangChain 极速入门"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" href="mailto:?subject=LangChain 极速入门&body=Check out this article: https://www.powercheng.fun/articles/2bf26df/" rel="external nofollow noreferrer"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://getpocket.com/save?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://reddit.com/submit?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.stumbleupon.com/submit?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="http://digg.com/submit?url=https://www.powercheng.fun/articles/2bf26df/&title=LangChain 极速入门"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" href="http://www.tumblr.com/share/link?url=https://www.powercheng.fun/articles/2bf26df/&name=LangChain 极速入门&description=&lt;link rel=&#34;stylesheet&#34; type=&#34;text/css&#34; href=&#34;https://cdn.jsdelivr.net/lightgallery.js/1.0.1/css/lightgallery.min.css&#34; /&gt;&lt;div class=&#34;.article-gallery&#34;&gt;&lt;p&gt;关于 LangChain 的 ChatModels、PromptTemplate、RAG、Agent 相关组件介绍&lt;/p&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li><li><a class="icon" target="_blank" rel="noopener external nofollow noreferrer" href="https://news.ycombinator.com/submitlink?u=https://www.powercheng.fun/articles/2bf26df/&t=LangChain 极速入门"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li></ul></div><div id="actions-footer"><a id="menu" class="icon" href="#" onclick='return $("#nav-footer").toggle(),!1'><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a> <a id="toc" class="icon" href="#" onclick='return $("#toc-footer").toggle(),!1'><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a> <a id="share" class="icon" href="#" onclick='return $("#share-footer").toggle(),!1'><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a> <a id="top" style="display:none" class="icon" href="#" onclick='$("html, body").animate({scrollTop:0},"fast")'><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a></div></div></div><footer id="footer"><div class="footer-left">Copyright &copy; 2021-2025 Caiden Hou</div><div class="footer-right"><nav><ul><li><a href="/">首页</a></li><li><a href="/archives/">文章</a></li><li><a href="/categories/">分类</a></li><li><a href="/search/">搜索</a></li><li><a href="/about/">关于</a></li></ul></nav></div></footer></div><link rel="preload" href="/lib/font-awesome/css/all.min.css" as="style" onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"></noscript><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript">$(function(){$(".highlight table").before('<span class="btn-copy tooltipped tooltipped-sw" aria-label="复制到粘贴板！"><i class="fa-regular fa-clone"></i></span>'),new ClipboardJS(".btn-copy",{text:function(e){return Array.from(e.nextElementSibling.querySelectorAll(".code")).reduce((e,t)=>e+t.innerText+"\n","")}}).on("success",function(e){e.trigger.setAttribute("aria-label","复制成功！"),e.clearSelection()})})</script><script src="/js/main.js"></script><script type="text/javascript">var disqus_shortname="caidens-blog";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script></body></html>